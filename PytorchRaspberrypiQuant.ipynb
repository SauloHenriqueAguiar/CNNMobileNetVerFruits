{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ESSE_TUTORIAL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "88c7cda5018c423b960e55586856d4f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d9c4478c06864835becb277a2a8c8c73",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0f09b4047c1c458a99b655e7307ea7fb",
              "IPY_MODEL_4fc83b0db4204bb1824bab8366decc3f",
              "IPY_MODEL_d17d0c5f211e45e785725b0cec7965a3"
            ]
          }
        },
        "d9c4478c06864835becb277a2a8c8c73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0f09b4047c1c458a99b655e7307ea7fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bb852894fdbe4fadb6aca00f1b195f27",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_299bcc8d238d4c07b0742e9c108d195b"
          }
        },
        "4fc83b0db4204bb1824bab8366decc3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a341017bae794d8fbf0570f7d5dd1b42",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0abcb26194324769855af5ee378fb31c"
          }
        },
        "d17d0c5f211e45e785725b0cec7965a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8934c1a5ff9646d891fd50220b33912d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:06&lt;00:00, 28228713.71it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_186a06133a544ae481b220664157759f"
          }
        },
        "bb852894fdbe4fadb6aca00f1b195f27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "299bcc8d238d4c07b0742e9c108d195b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a341017bae794d8fbf0570f7d5dd1b42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0abcb26194324769855af5ee378fb31c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8934c1a5ff9646d891fd50220b33912d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "186a06133a544ae481b220664157759f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_3WbBNZJ8w4"
      },
      "source": [
        "# **Deep Learning embedded into Raspberry PI 3 using Quantized Pytorch Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_4Ghw0IiSRI"
      },
      "source": [
        "3 diferentes técnicas de compressão de modelos utilizando diversos recursos que o framework Pytorch nos oferece. Iremos avaliar o ganho de perfomance em uma plataforma embarcada chamada Raspberry PI 3 e iremos estruturar como dar deploy desse modelo.\n",
        "\n",
        "### **Disclaimer:**\n",
        "Este tutorial se utiliza de diversos tutoriais online para sua criação. Abaixo há todos os links que os levam até eles.\n",
        "\n",
        "[Pytorch Quantize Recipe](https://pytorch.org/docs/stable/quantization.html#quantization-workflows)\n",
        "\n",
        "[Pytorch Quantization Docs](https://pytorch.org/docs/stable/quantization.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOdGoANzqG_0"
      },
      "source": [
        "## Relembrando \n",
        "\n",
        "Um neurônio é modelado como um produto interno: \n",
        "\n",
        "$y(\\textbf{X}) = \\beta^\\top \\textbf{X} = \\sum_i \\textbf{w}_i\\textbf{x}_i$\n",
        "\n",
        "obs: o bias está implícito no vetor de pesos.\n",
        "\n",
        "Dessa forma, como podemos treiná-lo?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzKJxNDSIwzl"
      },
      "source": [
        "# Treinando o modelo\n",
        "\n",
        "A função que atualiza os pesos pode ser calculada como:<br>\n",
        "<center>$\\begin{equation}\\Delta w_{i} =  - \\eta.\\nabla_{w}^E\\end{equation}$</center> \n",
        "\n",
        "Onde $\\nabla_{w}^E$ é o operador de gradiente calculado sobre a funação de erro em função dos pesos. Assumindo que cada peso é linearmente independente, podemos reescrever esta função como:<br>\n",
        "Sendo $\\textbf{w}$ um vetor de pesos e  $\\nabla_{\\textbf{w}}^E$ um vetor de derivadas parciais da função de erro em relação aos pesos, então:\n",
        "\n",
        "$\\begin{equation}\n",
        "  \\textbf{w} = \\begin{pmatrix} w_{1} \\\\ w_{2} \\\\ \\vdots \\\\ w_{n} \\end{pmatrix} ; \\nabla_{\\textbf{w}}^E = \\begin{pmatrix} \\frac{\\partial E}{\\partial w_{1}} \\\\ \\frac{\\partial E}{\\partial w_{2}}\\\\ \\vdots \\\\ \\frac{\\partial E}{\\partial w_{n}}\\end{pmatrix} \\end{equation}\n",
        "  $\n",
        "\n",
        "  Portanto $\\textbf{w}_{t+1} = \\textbf{w}_{t} - \\eta.\\nabla_{\\textbf{w}}^E$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q7tl3nxyJ5l"
      },
      "source": [
        "# **Convolutional Neural Networks**\n",
        "\n",
        "---\n",
        "\n",
        "Convolução em imagens é o processo de aplicar um filtro a uma representação da image. Em redes neurais, essas aplicações são compostas em diversas camadas onde diversos outros filtros são aplicados às representações anteriores da imagem até chegar a etapa de classificação. Essa parte da arquitetura é chamada de extrator de características.\n",
        "\n",
        "A etapa de classificação, geralmente, é composta por uma rede MLP ou fully connected.\n",
        "\n",
        "![conv 1](https://www.researchgate.net/publication/331540139/figure/fig4/AS:733273504354306@1551837435967/The-overall-architecture-of-the-Convolutional-Neural-Network-CNN-includes-an-input.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TArvE3mHqCE"
      },
      "source": [
        "#pip install torch==1.5.0\n",
        "#pip install torchvision==0.5.0\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rgnSCPpL43c",
        "outputId": "d1b0d3b2-5785-4e8f-ba12-955d7c646db3"
      },
      "source": [
        "#@title **Importando dependencias que serao utilizadas neste tutorial**\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "import torch.quantization\n",
        "from torch.utils.data import random_split\n",
        "from torch.quantization import QuantStub, DeQuantStub\n",
        "import copy\n",
        "# # Setup warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\n",
        "    action='ignore',\n",
        "    category=DeprecationWarning,\n",
        "    module=r'.*'\n",
        ")\n",
        "warnings.filterwarnings(\n",
        "    action='default',\n",
        "    module=r'torch.quantization'\n",
        ")\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown Para conseguirmos reproduzir aos experimentos de maneira deterministica,\n",
        "#@markdown precisamos configurar uma seed -->```torch.manual_seed(191009)```  \n",
        "# Specify random seed for repeatable results\n",
        "torch.manual_seed(191009)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fdc442f0570>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px2cHeB9HlDK"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "88c7cda5018c423b960e55586856d4f0",
            "d9c4478c06864835becb277a2a8c8c73",
            "0f09b4047c1c458a99b655e7307ea7fb",
            "4fc83b0db4204bb1824bab8366decc3f",
            "d17d0c5f211e45e785725b0cec7965a3",
            "bb852894fdbe4fadb6aca00f1b195f27",
            "299bcc8d238d4c07b0742e9c108d195b",
            "a341017bae794d8fbf0570f7d5dd1b42",
            "0abcb26194324769855af5ee378fb31c",
            "8934c1a5ff9646d891fd50220b33912d",
            "186a06133a544ae481b220664157759f"
          ]
        },
        "id": "k3x2R5bNq7cl",
        "outputId": "eaf4cb81-0d76-4182-d030-1e33e8282730"
      },
      "source": [
        "#@title Loading the dataset the same way as before, but now using Normalization\n",
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    \n",
        "])\n",
        "\n",
        "dataset_train = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "dataset_test = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "test, val = random_split(dataset_test, lengths = (5000,5000))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88c7cda5018c423b960e55586856d4f0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cyG-TXTq-Km"
      },
      "source": [
        "batch_size = 2500\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "train_loader = DataLoader(dataset=dataset_train, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(dataset=test, shuffle=False, batch_size=batch_size)\n",
        "val_loader = DataLoader(dataset=val, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "dataloaders = {'train': train_loader, 'val':val_loader, 'test' : test_loader }\n",
        "\n",
        "dataset_sizes = {'train' : len(train_loader.dataset), 'test' : len(test_loader.dataset), 'val': len(val_loader.dataset)}\n",
        "\n",
        "device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcaoDvPb7YQr"
      },
      "source": [
        "class ConvBNReLU(nn.Sequential):\n",
        "    def __init__(self, in_planes, out_planes, padding, kernel_size=3, stride=1, groups=1):\n",
        "        #padding = (kernel_size - 1) // 2\n",
        "        super(ConvBNReLU, self).__init__(\n",
        "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n",
        "            nn.BatchNorm2d(out_planes, momentum=0.1),\n",
        "            # Replace with ReLU\n",
        "            nn.ReLU(inplace=False)\n",
        "        )\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN,self).__init__()\n",
        "    \n",
        "    #input_channel, output_channel, feature_dimension(kernel_size), stride, padding\n",
        "    self.feats = nn.Sequential(\n",
        "        \n",
        "        ConvBNReLU(3, 20, kernel_size = 3, stride = 1, padding=0),#30x30x20\n",
        "        nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1),#15x15x20\n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "        ConvBNReLU(20, 256, kernel_size = 3, stride = 1, padding = 1),#15x15x256\n",
        "        nn.MaxPool2d(kernel_size = 3, stride = 2),#7x7x256\n",
        "      \n",
        "        nn.MaxPool2d(kernel_size = 3, stride = 2 )#3x3x256\n",
        "    )\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Dropout(0.4),\n",
        "        nn.Linear(3*3*256,10),\n",
        "        # nn.ReLU(True),\n",
        "        # nn.Dropout(0.5),\n",
        "        # nn.Linear(768, 10),\n",
        "        #nn.LogSoftmax(1)\n",
        "    )\n",
        "    \n",
        "    self.quant = QuantStub()\n",
        "    \n",
        "    self.dequant = DeQuantStub()\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.quant(x)\n",
        "    x = self.feats(x) # CNN\n",
        "\n",
        "    x = x.reshape(-1,3*3*256) # Lineariza \n",
        "\n",
        "    x = self.fc(x) #Classifica\n",
        "    x = self.dequant(x)\n",
        "    return x\n",
        "\n",
        "  def fuse_model(self):\n",
        "    for m in self.modules():\n",
        "        if type(m) == ConvBNReLU:\n",
        "            torch.quantization.fuse_modules(m, ['0', '1', '2'], inplace=True)\n",
        "        "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt8Umows1ETF"
      },
      "source": [
        "def train_model(model, criterion, optimizer, dataloaders, device='cpu', num_epochs=25):\n",
        "    since = time.time()\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            # if phase == 'train':\n",
        "            #     scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_HwgRANL-L5"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "\n",
        "def evaluate(model, criterion, data_loader, neval_batches):\n",
        "    model.eval()\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "    cnt = 0\n",
        "    with torch.no_grad():\n",
        "        for image, target in data_loader:\n",
        "            output = model(image)\n",
        "            loss = criterion(output, target)\n",
        "            cnt += 1\n",
        "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "            print('.', end = '')\n",
        "            top1.update(acc1[0], image.size(0))\n",
        "            top5.update(acc5[0], image.size(0))\n",
        "            if cnt >= neval_batches:\n",
        "                 return top1, top5\n",
        "\n",
        "    return top1, top5\n",
        "\n",
        "def load_model(model_file):\n",
        "    model = CNN()\n",
        "    state_dict = torch.load(model_file)\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.to('cpu')\n",
        "    return model\n",
        "\n",
        "def print_size_of_model(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
        "    os.remove('temp.p')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLN1Ef6W0psG",
        "outputId": "f210c7b1-b033-41ea-8e5d-a769bf49d45c"
      },
      "source": [
        "model = CNN()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "train_model(model.to(device), criterion, optimizer, dataloaders, device=device, num_epochs=3)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/2\n",
            "----------\n",
            "train Loss: 2.5618 Acc: 0.1436\n",
            "val Loss: 2.0909 Acc: 0.2708\n",
            "\n",
            "Epoch 1/2\n",
            "----------\n",
            "train Loss: 2.2057 Acc: 0.2259\n",
            "val Loss: 1.8730 Acc: 0.3490\n",
            "\n",
            "Epoch 2/2\n",
            "----------\n",
            "train Loss: 2.0512 Acc: 0.2767\n",
            "val Loss: 1.7733 Acc: 0.3810\n",
            "\n",
            "Training complete in 1m 13s\n",
            "Best val Acc: 0.381000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (feats): Sequential(\n",
              "    (0): ConvBNReLU(\n",
              "      (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (2): ConvBNReLU(\n",
              "      (0): Conv2d(20, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Dropout(p=0.4, inplace=False)\n",
              "    (1): Linear(in_features=2304, out_features=10, bias=True)\n",
              "  )\n",
              "  (quant): QuantStub()\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRYYioaL7PKx"
      },
      "source": [
        "torch.save(model.state_dict(), 'model_cnn_basic.pth')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYkKbR89fDdi"
      },
      "source": [
        "torch.jit.save(torch.jit.script(model.to('cpu')), 'model_cnn_basic_scripted.pth')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoO8QMCvMDdU"
      },
      "source": [
        "saved_model_dir = './'\n",
        "float_model_file = 'model_cnn_basic.pth'\n",
        "scripted_float_model_file = 'model_cnn_basic_quantization_scripted.pth'\n",
        "scripted_quantized_model_file = 'model_cnn_basic_quantization_scripted_quantized.pth'\n",
        "\n",
        "train_batch_size = 30\n",
        "eval_batch_size = 50\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "float_model = load_model(saved_model_dir + float_model_file).to('cpu')\n",
        "\n",
        "\n",
        "\n",
        "float_model.eval()\n",
        "\n",
        "# Fuses modules\n",
        "float_model.fuse_model()\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsFr_2IzMFeg",
        "outputId": "e49fb916-3f74-4b57-b277-505eb23ca72b"
      },
      "source": [
        "num_eval_batches = 1000\n",
        "\n",
        "print(\"Size of baseline model\")\n",
        "print_size_of_model(float_model)\n",
        "\n",
        "top1, top5 = evaluate(float_model, criterion, dataloaders['val'], neval_batches=num_eval_batches)\n",
        "print('\\nEvaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
        "torch.jit.save(torch.jit.script(float_model), saved_model_dir + scripted_float_model_file)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of baseline model\n",
            "Size (MB): 0.282135\n",
            "..\n",
            "Evaluation accuracy on 50000 images, 38.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvQy3dFoMI02",
        "outputId": "b890d677-d2a1-4e0b-8ab7-0f00f1caf28e"
      },
      "source": [
        "num_calibration_batches = 32\n",
        "\n",
        "myModel = load_model(saved_model_dir + float_model_file).to('cpu')\n",
        "myModel.eval()\n",
        "\n",
        "# Fuse Conv, bn and relu\n",
        "myModel.fuse_model()\n",
        "\n",
        "# Specify quantization configuration\n",
        "# Start with simple min/max range estimation and per-tensor quantization of weights\n",
        "myModel.qconfig = torch.quantization.default_qconfig\n",
        "print(myModel.qconfig)\n",
        "backend = \"qnnpack\"\n",
        "\n",
        "torch.backends.quantized.engine = backend\n",
        "torch.quantization.prepare(myModel, inplace=True)\n",
        "\n",
        "# Calibrate first\n",
        "print('Post Training Quantization Prepare: Inserting Observers')\n",
        "#print('\\n Inverted Residual Block:After observer insertion \\n\\n', myModel.features[1].conv)\n",
        "\n",
        "# Calibrate with the training set\n",
        "evaluate(myModel, criterion, dataloaders['val'], neval_batches=num_calibration_batches)\n",
        "print('Post Training Quantization: Calibration done')\n",
        "\n",
        "# Convert to quantized model\n",
        "torch.quantization.convert(myModel, inplace=True)\n",
        "print('Post Training Quantization: Convert done')\n",
        "#print('\\n Inverted Residual Block: After fusion and quantization, note fused modules: \\n\\n',myModel.features[1].conv)\n",
        "\n",
        "print(\"Size of model after quantization\")\n",
        "print_size_of_model(myModel)\n",
        "\n",
        "top1, top5 = evaluate(myModel, criterion, dataloaders['val'], neval_batches=num_eval_batches)\n",
        "print('\\nEvaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
            "Post Training Quantization Prepare: Inserting Observers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:174: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  reduce_range will be deprecated in a future release of PyTorch.\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "..Post Training Quantization: Calibration done\n",
            "Post Training Quantization: Convert done\n",
            "Size of model after quantization\n",
            "Size (MB): 0.075451\n",
            "..\n",
            "Evaluation accuracy on 50000 images, 37.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVLeK-uYMKfG",
        "outputId": "268cdde7-0c5b-4d86-8a73-9fbc3c0b435f"
      },
      "source": [
        "per_channel_quantized_model = load_model(saved_model_dir + float_model_file)\n",
        "per_channel_quantized_model.eval()\n",
        "per_channel_quantized_model.fuse_model()\n",
        "per_channel_quantized_model.qconfig = torch.quantization.get_default_qconfig('qnnpack')\n",
        "print(per_channel_quantized_model.qconfig)\n",
        "\n",
        "torch.quantization.prepare(per_channel_quantized_model, inplace=True)\n",
        "evaluate(per_channel_quantized_model,criterion, dataloaders['val'], num_calibration_batches)\n",
        "torch.quantization.convert(per_channel_quantized_model, inplace=True)\n",
        "top1, top5 = evaluate(per_channel_quantized_model, criterion, dataloaders['val'], neval_batches=num_eval_batches)\n",
        "print('\\nEvaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
        "torch.jit.save(torch.jit.script(per_channel_quantized_model), saved_model_dir + scripted_quantized_model_file)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=False){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
            ".."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:886: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  src_bin_begin // dst_bin_width, 0, self.dst_nbins - 1\n",
            "/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:891: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  src_bin_end // dst_bin_width, 0, self.dst_nbins - 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "..\n",
            "Evaluation accuracy on 50000 images, 38.04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3xQMf6JMMWK"
      },
      "source": [
        "def train_one_epoch(model, criterion, optimizer, data_loader, device, ntrain_batches):\n",
        "    model.train()\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "    avgloss = AverageMeter('Loss', '1.5f')\n",
        "\n",
        "    cnt = 0\n",
        "    for image, target in data_loader:\n",
        "        start_time = time.time()\n",
        "        print('.', end = '')\n",
        "        cnt += 1\n",
        "        image, target = image.to(device), target.to(device)\n",
        "        output = model(image)\n",
        "        loss = criterion(output, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "        top1.update(acc1[0], image.size(0))\n",
        "        top5.update(acc5[0], image.size(0))\n",
        "        avgloss.update(loss, image.size(0))\n",
        "        if cnt >= ntrain_batches:\n",
        "            print('Loss', avgloss.avg)\n",
        "\n",
        "            print('Training: * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
        "                  .format(top1=top1, top5=top5))\n",
        "            return\n",
        "\n",
        "    print('Full CIFAR10 train set:  * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
        "          .format(top1=top1, top5=top5))\n",
        "    return"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZku6BJjMObg"
      },
      "source": [
        "qat_model = load_model(saved_model_dir + float_model_file)\n",
        "qat_model.fuse_model()\n",
        "\n",
        "optimizer = torch.optim.SGD(qat_model.parameters(), lr = 0.0001)\n",
        "qat_model.qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeR3gzkAMRs3",
        "outputId": "da8bff42-ed28-4479-d745-e65687c0ec52"
      },
      "source": [
        "torch.quantization.prepare_qat(qat_model, inplace=True)\n",
        "#print('Inverted Residual Block: After preparation for QAT, note fake-quantization modules \\n',qat_model.features[1].conv)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (feats): Sequential(\n",
              "    (0): ConvBNReLU(\n",
              "      (0): ConvBnReLU2d(\n",
              "        3, 20, kernel_size=(3, 3), stride=(1, 1), bias=False\n",
              "        (bn): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "      (1): Identity()\n",
              "      (2): Identity()\n",
              "    )\n",
              "    (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (2): ConvBNReLU(\n",
              "      (0): ConvBnReLU2d(\n",
              "        20, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
              "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "      (1): Identity()\n",
              "      (2): Identity()\n",
              "    )\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Dropout(p=0.4, inplace=False)\n",
              "    (1): Linear(\n",
              "      in_features=2304, out_features=10, bias=True\n",
              "      (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "      )\n",
              "      (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (quant): QuantStub(\n",
              "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "  )\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W43dNZ_WMVWc",
        "outputId": "5a1c984b-9fe7-4b43-a23d-e17f28ed9111"
      },
      "source": [
        "def run_benchmark(model_file, img_loader):\n",
        "    elapsed = 0\n",
        "    model = torch.jit.load(model_file)\n",
        "    model.eval()\n",
        "    num_batches = 1\n",
        "    # Run the scripted model on a few batches of images\n",
        "    for i, (images, target) in enumerate(img_loader):\n",
        "        if i < num_batches:\n",
        "            start = time.time()\n",
        "            output = model(images)\n",
        "            end = time.time()\n",
        "            elapsed = elapsed + (end-start)\n",
        "        else:\n",
        "            break\n",
        "    num_images = images.size()[0] * num_batches\n",
        "\n",
        "    print('Elapsed time: %3.3f ms' % (elapsed/num_images*1000))\n",
        "    return elapsed\n",
        "\n",
        "run_benchmark(saved_model_dir + scripted_float_model_file, dataloaders['val'])\n",
        "\n",
        "run_benchmark(saved_model_dir + scripted_quantized_model_file, dataloaders['val'])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time: 1.411 ms\n",
            "Elapsed time: 1.344 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.360507011413574"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7uT-vu5MThN",
        "outputId": "84404afe-6f8a-42b4-9ce6-5fc39292329f"
      },
      "source": [
        "num_train_batches = 20\n",
        "\n",
        "# QAT takes time and one needs to train over a few epochs.\n",
        "# Train and check accuracy after each epoch\n",
        "for nepoch in range(8):\n",
        "    train_one_epoch(qat_model, criterion, optimizer, dataloaders['test'], torch.device('cpu'), num_train_batches)\n",
        "    if nepoch > 3:\n",
        "        # Freeze quantizer parameters\n",
        "        qat_model.apply(torch.quantization.disable_observer)\n",
        "    if nepoch > 2:\n",
        "        # Freeze batch norm mean and variance estimates\n",
        "        qat_model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n",
        "\n",
        "    # Check the accuracy after each epoch\n",
        "    quantized_model = torch.quantization.convert(qat_model.eval(), inplace=False)\n",
        "    quantized_model.eval()\n",
        "    top1, top5 = evaluate(quantized_model,criterion, dataloaders['val'], neval_batches=num_eval_batches)\n",
        "    print('\\nEpoch %d :Evaluation accuracy on %d images, %2.2f'%(nepoch, num_eval_batches * eval_batch_size, top1.avg))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "..Full CIFAR10 train set:  * Acc@1 29.000 Acc@5 80.140\n",
            "..\n",
            "Epoch 0 :Evaluation accuracy on 50000 images, 38.12\n",
            "..Full CIFAR10 train set:  * Acc@1 28.060 Acc@5 79.720\n",
            "..\n",
            "Epoch 1 :Evaluation accuracy on 50000 images, 38.14\n",
            "..Full CIFAR10 train set:  * Acc@1 30.060 Acc@5 80.760\n",
            "..\n",
            "Epoch 2 :Evaluation accuracy on 50000 images, 38.28\n",
            "..Full CIFAR10 train set:  * Acc@1 29.180 Acc@5 80.560\n",
            "..\n",
            "Epoch 3 :Evaluation accuracy on 50000 images, 38.06\n",
            "..Full CIFAR10 train set:  * Acc@1 29.100 Acc@5 79.740\n",
            "..\n",
            "Epoch 4 :Evaluation accuracy on 50000 images, 38.14\n",
            "..Full CIFAR10 train set:  * Acc@1 29.220 Acc@5 80.920\n",
            "..\n",
            "Epoch 5 :Evaluation accuracy on 50000 images, 38.06\n",
            "..Full CIFAR10 train set:  * Acc@1 30.260 Acc@5 80.740\n",
            "..\n",
            "Epoch 6 :Evaluation accuracy on 50000 images, 38.08\n",
            "..Full CIFAR10 train set:  * Acc@1 29.700 Acc@5 81.120\n",
            "..\n",
            "Epoch 7 :Evaluation accuracy on 50000 images, 38.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAuq-w_tpvq1",
        "outputId": "166e4ac4-e5ae-48c1-8b9f-da9a6561802c"
      },
      "source": [
        "torch.quantization.convert(quantized_model, inplace=True)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (feats): Sequential(\n",
              "    (0): ConvBNReLU(\n",
              "      (0): QuantizedConvReLU2d(3, 20, kernel_size=(3, 3), stride=(1, 1), scale=0.025057807564735413, zero_point=0)\n",
              "      (1): Identity()\n",
              "      (2): Identity()\n",
              "    )\n",
              "    (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (2): ConvBNReLU(\n",
              "      (0): QuantizedConvReLU2d(20, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.03023986704647541, zero_point=0, padding=(1, 1))\n",
              "      (1): Identity()\n",
              "      (2): Identity()\n",
              "    )\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Dropout(p=0.4, inplace=False)\n",
              "    (1): QuantizedLinear(in_features=2304, out_features=10, scale=0.049142949283123016, zero_point=106, qscheme=torch.per_tensor_affine)\n",
              "  )\n",
              "  (quant): Quantize(scale=tensor([0.0187]), zero_point=tensor([114]), dtype=torch.quint8)\n",
              "  (dequant): DeQuantize()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBRK7tg7Kd9D"
      },
      "source": [
        "torch.jit.save(torch.jit.script(quantized_model), 'qat_model.pth')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj97_PtGURzA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}